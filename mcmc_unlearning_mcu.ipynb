{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08630b62",
   "metadata": {},
   "source": [
    "# MCMC-Based Machine Unlearning (MCU)\n",
    "### Replication of: *\"Markov Chain Monte Carlo-Based Machine Unlearning: Unlearning what needs to be forgotten\"* ‚Äî ASIACCS 2022\n",
    "\n",
    "---\n",
    "\n",
    "**Machine Unlearning** is the problem of removing the influence of specific training points from a trained model *without retraining from scratch*.\n",
    "\n",
    "### Why does it matter?\n",
    "| Motivation | Example |\n",
    "|---|---|\n",
    "| üîí Right to be Forgotten (GDPR/CCPA) | A user requests their data be deleted |\n",
    "| ‚ò£Ô∏è Poisoning attack mitigation | Remove adversarially injected samples |\n",
    "| ‚úèÔ∏è Data correction | Fix mislabeled / corrupted training data |\n",
    "\n",
    "### MCU Core Idea\n",
    "Instead of full retraining, MCU uses **Bayesian Importance Sampling** to reweight the existing posterior:\n",
    "\n",
    "$$P(\\theta \\mid D_r) \\propto \\frac{P(\\theta \\mid D)}{P(D_e \\mid \\theta)}$$\n",
    "\n",
    "This notebook walks through every step: dataset setup ‚Üí MCMC training ‚Üí unlearning ‚Üí evaluation & diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bf929",
   "metadata": {},
   "source": [
    "## Section 1 ‚Äî Setup & Imports\n",
    "\n",
    "Install and import all required libraries. JAX provides GPU-accelerated numerical computing; NumPyro is the probabilistic programming framework built on JAX; scikit-learn handles dataset generation and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"JAX version  : {jax.__version__}\")\n",
    "print(f\"NumPyro ver  : {numpyro.__version__}\")\n",
    "print(f\"JAX backend  : {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324cb95",
   "metadata": {},
   "source": [
    "## Section 2 ‚Äî Dataset Generation\n",
    "\n",
    "We generate a synthetic tabular binary classification dataset designed to mimic the [UCI Phishing Websites Dataset](https://archive.ics.uci.edu/ml/datasets/phishing+websites).\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| Total samples | 1000 |\n",
    "| Features | 15 (10 informative, 2 redundant, 3 noise) |\n",
    "| Class separation | 1.2 |\n",
    "| Train / Test split | 80% / 20% |\n",
    "| Task | Binary classification: `0` = legitimate, `1` = phishing |\n",
    "\n",
    "`make_classification` creates correlated, realistic feature structures ‚Äî not a trivially separable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8401ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=1000, n_features=15, random_state=42):\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=10,\n",
    "        n_redundant=2,\n",
    "        class_sep=1.2,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_dataset()\n",
    "D_test = (X_test, y_test)\n",
    "\n",
    "print(f\"Training samples : {X_train.shape[0]}\")\n",
    "print(f\"Test samples     : {X_test.shape[0]}\")\n",
    "print(f\"Features         : {X_train.shape[1]}\")\n",
    "print(f\"Class balance    : train {np.bincount(y_train)}, test {np.bincount(y_test)}\")\n",
    "\n",
    "# --- Visualization: Class distribution + PCA overview ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "# Class bar chart\n",
    "axes[0].bar([\"Class 0 (legit)\", \"Class 1 (phish)\"], np.bincount(y_train),\n",
    "            color=[\"#4C72B0\", \"#DD8452\"], edgecolor=\"white\")\n",
    "axes[0].set_title(\"Training Set Class Distribution\", fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "for i, v in enumerate(np.bincount(y_train)):\n",
    "    axes[0].text(i, v + 5, str(v), ha=\"center\", fontsize=11)\n",
    "\n",
    "# PCA 2D scatter of all training data\n",
    "pca_vis = PCA(n_components=2)\n",
    "X_2d_vis = pca_vis.fit_transform(X_train)\n",
    "var_vis = pca_vis.explained_variance_ratio_\n",
    "for cls, color, label in [(0, \"#4C72B0\", \"Class 0\"), (1, \"#DD8452\", \"Class 1\")]:\n",
    "    mask = y_train == cls\n",
    "    axes[1].scatter(X_2d_vis[mask, 0], X_2d_vis[mask, 1],\n",
    "                    c=color, alpha=0.4, s=15, label=label)\n",
    "axes[1].set_title(\"Full Training Set ‚Äî PCA 2D Projection\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(f\"PC1 ({var_vis[0]*100:.1f}% var)\")\n",
    "axes[1].set_ylabel(f\"PC2 ({var_vis[1]*100:.1f}% var)\")\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c93b91",
   "metadata": {},
   "source": [
    "## Section 3 ‚Äî Erase Set Configuration\n",
    "\n",
    "The training data is split into two disjoint sets:\n",
    "\n",
    "| Symbol | Name | Description |\n",
    "|---|---|---|\n",
    "| $D_r$ | **Retain set** | Data the model should keep knowing |\n",
    "| $D_e$ | **Erase set** | Data whose influence must be removed |\n",
    "\n",
    "### Two Deletion Modes\n",
    "\n",
    "**`random`** ‚Äî Remove 5 uniformly random training points.  \n",
    "Simulates a single user data deletion request. Small, scattered erase set ‚Üí easy for importance sampling.\n",
    "\n",
    "**`cluster`** ‚Äî Remove all points where `feature_0 > 90th percentile` (~80 points, ~10% of data).  \n",
    "Simulates removing a structured demographic group or a poisoning cluster. Large, geometrically concentrated erase set ‚Üí harder for importance sampling (risk of weight degeneracy).\n",
    "\n",
    "> **Toggle `active_mode` below to switch between the two scenarios.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eff40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_erase_data(X_train, y_train, mode=\"random\", seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n_train = len(X_train)\n",
    "    if mode == \"random\":\n",
    "        erase_indices = np.random.choice(n_train, 5, replace=False)\n",
    "    elif mode == \"cluster\":\n",
    "        threshold = np.percentile(X_train[:, 0], 90)\n",
    "        erase_indices = np.where(X_train[:, 0] > threshold)[0]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mode.\")\n",
    "    erase_mask = np.zeros(n_train, dtype=bool)\n",
    "    erase_mask[erase_indices] = True\n",
    "    D_e = (X_train[erase_mask],  y_train[erase_mask])\n",
    "    D_r = (X_train[~erase_mask], y_train[~erase_mask])\n",
    "    return D_e, D_r\n",
    "\n",
    "# ============================================================\n",
    "#  TOGGLE HERE:  \"random\"  or  \"cluster\"\n",
    "active_mode = \"random\"\n",
    "# ============================================================\n",
    "\n",
    "D_e, D_r = split_erase_data(X_train, y_train, mode=active_mode)\n",
    "X_r, y_r = D_r\n",
    "X_e, y_e = D_e\n",
    "\n",
    "print(f\"Mode          : {active_mode.upper()}\")\n",
    "print(f\"Retain set D_r: {len(X_r)} samples\")\n",
    "print(f\"Erase  set D_e: {len(X_e)} samples  ({len(X_e)/len(X_train)*100:.1f}% of training data)\")\n",
    "\n",
    "# --- PCA scatter: Retain vs Erase ---\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "X_all_combined = np.vstack([X_r, X_e])\n",
    "y_all_combined = np.concatenate([y_r, y_e])\n",
    "labels_combined = np.array([\"Retain\"] * len(X_r) + [\"Erase\"] * len(X_e))\n",
    "\n",
    "pca_split = PCA(n_components=2)\n",
    "X_2d_split = pca_split.fit_transform(X_all_combined)\n",
    "var_split = pca_split.explained_variance_ratio_\n",
    "\n",
    "style_map = {(0, \"Retain\"): (\"o\", \"#4C72B0\", 0.35, 18),\n",
    "             (1, \"Retain\"): (\"s\", \"#DD8452\", 0.35, 18),\n",
    "             (0, \"Erase\"):  (\"o\", \"red\",     0.90, 80),\n",
    "             (1, \"Erase\"):  (\"s\", \"red\",     0.90, 80)}\n",
    "\n",
    "for (cls, grp), (mrk, col, alp, sz) in style_map.items():\n",
    "    mask = (y_all_combined == cls) & (labels_combined == grp)\n",
    "    if mask.any():\n",
    "        ax.scatter(X_2d_split[mask, 0], X_2d_split[mask, 1],\n",
    "                   marker=mrk, c=col, alpha=alp, s=sz,\n",
    "                   edgecolors=\"black\" if grp == \"Erase\" else \"none\",\n",
    "                   linewidths=0.7,\n",
    "                   label=f\"{grp} cls={cls}\")\n",
    "\n",
    "ax.set_title(f\"Dataset Split ‚Äî {active_mode.upper()} Mode (PCA 2D)\", fontweight=\"bold\")\n",
    "ax.set_xlabel(f\"PC1 ({var_split[0]*100:.1f}% var)\")\n",
    "ax.set_ylabel(f\"PC2 ({var_split[1]*100:.1f}% var)\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.text(0.02, 0.98, f\"D_r={len(X_r)}  D_e={len(X_e)}\",\n",
    "        transform=ax.transAxes, va=\"top\", fontsize=9,\n",
    "        bbox=dict(boxstyle=\"round\", alpha=0.2))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662b2fb",
   "metadata": {},
   "source": [
    "## Section 4 ‚Äî Bayesian Logistic Regression Model\n",
    "\n",
    "### Model Definition\n",
    "\n",
    "We place independent **standard Normal priors** on all parameters:\n",
    "\n",
    "$$P(w) = \\prod_{j=1}^{15} \\mathcal{N}(w_j;\\, 0,\\, 1), \\qquad P(b) = \\mathcal{N}(b;\\, 0,\\, 1)$$\n",
    "\n",
    "The **likelihood** for each training point uses the Bernoulli distribution with sigmoid-activated logits:\n",
    "\n",
    "$$P(y_i = 1 \\mid x_i,\\, w,\\, b) = \\sigma(w^\\top x_i + b), \\qquad \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The joint **posterior** over the full training dataset $D$:\n",
    "\n",
    "$$P(\\theta \\mid D) \\propto \\underbrace{P(D \\mid \\theta)}_{\\text{likelihood}} \\cdot \\underbrace{P(\\theta)}_{\\text{prior}} = \\prod_{i=1}^{n} \\text{Bernoulli}(y_i \\mid \\sigma(\\theta^\\top x_i)) \\cdot P(\\theta)$$\n",
    "\n",
    "NUTS (No-U-Turn Sampler) draws samples from this posterior using Hamiltonian Monte Carlo with automatic step-size tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_logistic_regression(X, y=None):\n",
    "    \"\"\"NumPyro Bayesian Logistic Regression model.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = numpyro.sample(\"w\", dist.Normal(jnp.zeros(n_features), jnp.ones(n_features)))\n",
    "    b = numpyro.sample(\"b\", dist.Normal(0.0, 1.0))\n",
    "    logits = jnp.matmul(X, w) + b\n",
    "    with numpyro.plate(\"data\", n_samples):\n",
    "        numpyro.sample(\"obs\", dist.Bernoulli(logits=logits), obs=y)\n",
    "\n",
    "def train_mcmc(X, y, num_warmup=500, num_samples=1000, rng_key=random.PRNGKey(0)):\n",
    "    \"\"\"Runs NUTS MCMC and returns posterior samples.\"\"\"\n",
    "    kernel = NUTS(bayesian_logistic_regression)\n",
    "    mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples, progress_bar=True)\n",
    "    mcmc.run(rng_key, X=jnp.array(X, dtype=jnp.float32),\n",
    "                      y=jnp.array(y, dtype=jnp.float32))\n",
    "    return mcmc.get_samples()\n",
    "\n",
    "# --- Visualize the Standard Normal prior ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "z = np.linspace(-4, 4, 400)\n",
    "prior_pdf = scipy.stats.norm.pdf(z, 0, 1)\n",
    "\n",
    "axes[0].plot(z, prior_pdf, color=\"#4C72B0\", linewidth=2.5)\n",
    "axes[0].fill_between(z, prior_pdf, alpha=0.15, color=\"#4C72B0\")\n",
    "axes[0].axvline(0, color=\"grey\", linestyle=\"--\", alpha=0.6)\n",
    "axes[0].set_title(\"Prior: $w_j \\\\sim \\\\mathcal{N}(0, 1)$\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Parameter value\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].text(-3.8, 0.35, \"Regularises weights\\ntoward zero\", fontsize=9, color=\"#4C72B0\")\n",
    "\n",
    "# Sigmoid function\n",
    "z_sig = np.linspace(-6, 6, 400)\n",
    "axes[1].plot(z_sig, 1/(1+np.exp(-z_sig)), color=\"#DD8452\", linewidth=2.5)\n",
    "axes[1].axhline(0.5, color=\"red\", linestyle=\"--\", linewidth=1, label=\"Decision boundary (0.5)\")\n",
    "axes[1].axvline(0, color=\"grey\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1].set_title(\"Likelihood Link: $\\\\sigma(z) = 1/(1+e^{-z})$\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Logit $z = w^\\\\top x + b$\")\n",
    "axes[1].set_ylabel(\"$P(y=1 \\\\mid x, \\\\theta)$\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Model defined. Ready for MCMC training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8fe2f9",
   "metadata": {},
   "source": [
    "## Section 5 ‚Äî MCMC Training: Base Model on Full Dataset $D$\n",
    "\n",
    "We train the base model on the **full dataset** $D = D_r \\cup D_e$ using the NUTS sampler.\n",
    "\n",
    "### NUTS Configuration\n",
    "| Parameter | Value | Purpose |\n",
    "|---|---|---|\n",
    "| `num_warmup` | 500 | Tuning phase ‚Äî step size & mass matrix adaptation (discarded) |\n",
    "| `num_samples` | 1500 | Retained posterior samples used for inference |\n",
    "| `rng_key` | PRNGKey(1) | Deterministic seeding |\n",
    "\n",
    "### What is NUTS?\n",
    "NUTS (No-U-Turn Sampler) extends HMC by automatically choosing trajectory lengths. It simulates Hamiltonian dynamics in the parameter space using the **gradient of the log-posterior** to make efficient, large, directed steps across the posterior ‚Äî far more efficient than random-walk Metropolis.\n",
    "\n",
    "The MCMC trace below should show **good mixing** ‚Äî a stationary, rapidly oscillating chain with no obvious trends or stuck regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Base Model on full dataset D = D_r ‚à™ D_e ...\")\n",
    "X_full = np.vstack((X_r, X_e))\n",
    "y_full = np.concatenate((y_r, y_e))\n",
    "\n",
    "base_samples = train_mcmc(X_full, y_full, num_warmup=500, num_samples=1500,\n",
    "                           rng_key=random.PRNGKey(1))\n",
    "\n",
    "print(f\"\\nPosterior sample shapes:\")\n",
    "print(f\"  w : {base_samples['w'].shape}   (num_samples √ó n_features)\")\n",
    "print(f\"  b : {base_samples['b'].shape}   (num_samples,)\")\n",
    "\n",
    "# --- MCMC Trace plots ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 6))\n",
    "fig.suptitle(\"Base Model ‚Äî MCMC Traces (chain health check)\", fontweight=\"bold\")\n",
    "\n",
    "b_base = np.array(base_samples[\"b\"])\n",
    "w_base = np.array(base_samples[\"w\"])\n",
    "\n",
    "# Top row: bias b trace + histogram\n",
    "axes[0, 0].plot(b_base, color=\"#4C72B0\", linewidth=0.5, alpha=0.8)\n",
    "axes[0, 0].axhline(b_base.mean(), color=\"crimson\", linestyle=\"--\", linewidth=1.5,\n",
    "                   label=f\"Mean = {b_base.mean():.3f}\")\n",
    "axes[0, 0].set_title(\"Trace ‚Äî Bias $b$\")\n",
    "axes[0, 0].set_xlabel(\"Sample index\")\n",
    "axes[0, 0].set_ylabel(\"$b$\")\n",
    "axes[0, 0].legend(fontsize=8)\n",
    "\n",
    "axes[0, 1].hist(b_base, bins=40, color=\"#4C72B0\", edgecolor=\"white\", linewidth=0.4)\n",
    "axes[0, 1].axvline(b_base.mean(), color=\"crimson\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[0, 1].set_title(\"Marginal Posterior ‚Äî Bias $b$\")\n",
    "axes[0, 1].set_xlabel(\"$b$\")\n",
    "axes[0, 1].set_ylabel(\"Count\")\n",
    "\n",
    "# Bottom row: w[0] trace + histogram\n",
    "top_dim = int(np.argmax(np.abs(w_base.mean(axis=0))))\n",
    "axes[1, 0].plot(w_base[:, top_dim], color=\"#DD8452\", linewidth=0.5, alpha=0.8)\n",
    "axes[1, 0].axhline(w_base[:, top_dim].mean(), color=\"crimson\", linestyle=\"--\",\n",
    "                   linewidth=1.5, label=f\"Mean = {w_base[:, top_dim].mean():.3f}\")\n",
    "axes[1, 0].set_title(f\"Trace ‚Äî Weight $w[{top_dim}]$ (most influential)\")\n",
    "axes[1, 0].set_xlabel(\"Sample index\")\n",
    "axes[1, 0].set_ylabel(f\"$w[{top_dim}]$\")\n",
    "axes[1, 0].legend(fontsize=8)\n",
    "\n",
    "axes[1, 1].hist(w_base[:, top_dim], bins=40, color=\"#DD8452\", edgecolor=\"white\", linewidth=0.4)\n",
    "axes[1, 1].axvline(w_base[:, top_dim].mean(), color=\"crimson\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[1, 1].set_title(f\"Marginal Posterior ‚Äî $w[{top_dim}]$\")\n",
    "axes[1, 1].set_xlabel(f\"$w[{top_dim}]$\")\n",
    "axes[1, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eb438f",
   "metadata": {},
   "source": [
    "## Section 6 ‚Äî MCU Unlearning via Importance Sampling\n",
    "\n",
    "### The Core Mathematical Insight\n",
    "\n",
    "Using Bayes' rule, the target retrained posterior can be expressed as:\n",
    "\n",
    "$$P(\\theta \\mid D_r) = \\frac{P(\\theta \\mid D)}{P(D_e \\mid \\theta)} \\cdot Z^{-1}$$\n",
    "\n",
    "This means we can **reweight** the existing MCMC samples from $P(\\theta \\mid D)$ rather than resampling from scratch.\n",
    "\n",
    "### Importance Weight Formula\n",
    "\n",
    "For each posterior sample $\\theta_i$:\n",
    "\n",
    "$$\\log w_i = -\\log P(D_e \\mid \\theta_i) = -\\sum_{(x, y) \\in D_e} \\log \\text{Bernoulli}\\!\\left(y \\;\\middle|\\; \\sigma(\\theta_i^\\top x)\\right)$$\n",
    "\n",
    "Normalized (via log-sum-exp to prevent numerical overflow):\n",
    "\n",
    "$$\\tilde{w}_i = \\text{softmax}(\\log w)_i = \\frac{e^{\\log w_i}}{\\sum_j e^{\\log w_j}}$$\n",
    "\n",
    "### Effective Sample Size (ESS)\n",
    "\n",
    "$$\\text{ESS} = \\frac{1}{\\sum_{i=1}^N \\tilde{w}_i^2}$$\n",
    "\n",
    "- **ESS ‚âà N**: Weights are uniform ‚Üí the unlearning barely changed the posterior (small D_e)\n",
    "- **ESS ‚â™ N**: Weight degeneracy ‚Üí only a few samples dominate ‚Üí unlearning quality degrades\n",
    "\n",
    "### Intuition for weights\n",
    "| Sample $\\theta_i$ | Likelihood $P(D_e|\\theta_i)$ | Weight $\\tilde{w}_i$ | Meaning |\n",
    "|---|---|---|---|\n",
    "| Fits D_e poorly | Low | **High** | This sample is consistent with never having seen D_e |\n",
    "| Fits D_e well | High | **Low** | This sample \"remembers\" D_e ‚Äî de-emphasize it |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b784ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_unlearning(D_e, posterior_samples):\n",
    "    \"\"\"\n",
    "    MCU: compute normalized importance weights wÃÉ_i = 1/P(D_e|Œ∏_i).\n",
    "    Returns a (num_samples,) array of normalized weights.\n",
    "    \"\"\"\n",
    "    X_e, y_e = D_e\n",
    "    w = posterior_samples['w']   # (N, d)\n",
    "    b = posterior_samples['b']   # (N,)\n",
    "\n",
    "    logits = jnp.dot(w, X_e.T) + b[:, None]        # (N, |D_e|)\n",
    "    log_liks = dist.Bernoulli(logits=logits).log_prob(y_e)  # (N, |D_e|)\n",
    "    total_log_lik = jnp.sum(log_liks, axis=1)       # (N,)\n",
    "\n",
    "    log_weights = -total_log_lik                     # negate to get 1/P\n",
    "    log_weights_norm = log_weights - jax.nn.logsumexp(log_weights)\n",
    "    return jnp.exp(log_weights_norm)\n",
    "\n",
    "print(\"Applying MCU Importance Sampling...\")\n",
    "unlearning_weights = perform_unlearning(D_e, base_samples)\n",
    "weights_np = np.array(unlearning_weights)\n",
    "\n",
    "N = len(weights_np)\n",
    "ess = 1.0 / np.sum(weights_np ** 2)\n",
    "ess_pct = ess / N * 100\n",
    "print(f\"  N samples             : {N}\")\n",
    "print(f\"  ESS                   : {ess:.1f}  ({ess_pct:.1f}% of N)\")\n",
    "print(f\"  Max weight            : {weights_np.max():.6f}\")\n",
    "print(f\"  Uniform weight (1/N)  : {1/N:.6f}\")\n",
    "if ess_pct < 10:\n",
    "    print(\"  ‚ö† WARNING: ESS < 10% ‚Äî weight degeneracy detected!\")\n",
    "    print(\"    Consider using the Enlarged Candidate Set approach.\")\n",
    "\n",
    "# --- Importance weight histogram ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].hist(weights_np, bins=60, color=\"steelblue\", edgecolor=\"white\", linewidth=0.4)\n",
    "axes[0].axvline(1.0/N, color=\"crimson\", linestyle=\"--\", linewidth=1.8,\n",
    "                label=f\"Uniform (1/N = {1/N:.5f})\")\n",
    "axes[0].set_title(\"Importance Weight Distribution\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Normalized weight $\\\\tilde{w}_i$\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].legend()\n",
    "axes[0].text(0.97, 0.93, f\"ESS = {ess:.1f}/{N}\\n({ess_pct:.1f}%)\",\n",
    "             transform=axes[0].transAxes, ha=\"right\", va=\"top\",\n",
    "             color=\"darkred\", fontweight=\"bold\", fontsize=10,\n",
    "             bbox=dict(boxstyle=\"round\", alpha=0.15))\n",
    "if ess_pct < 10:\n",
    "    axes[0].set_facecolor(\"#fff0f0\")\n",
    "    axes[0].text(0.5, 0.5, \"‚ö† Weight Degeneracy\",\n",
    "                 transform=axes[0].transAxes, fontsize=11, ha=\"center\",\n",
    "                 color=\"red\", alpha=0.35, fontweight=\"bold\", rotation=12)\n",
    "\n",
    "# Log-weight distribution\n",
    "log_w_raw = np.array(-jnp.sum(\n",
    "    dist.Bernoulli(logits=jnp.dot(base_samples['w'], X_e.T) +\n",
    "                   base_samples['b'][:, None]).log_prob(y_e), axis=1))\n",
    "axes[1].hist(log_w_raw, bins=60, color=\"#9C27B0\", edgecolor=\"white\", linewidth=0.4)\n",
    "axes[1].set_title(\"Raw Log-Weights $\\\\log w_i = -\\\\log P(D_e|\\\\theta_i)$\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"$\\\\log w_i$  (higher = less fit to $D_e$)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].text(0.03, 0.95,\n",
    "             \"High log-weight samples\\nconsistent with forgetting D_e\",\n",
    "             transform=axes[1].transAxes, fontsize=8, va=\"top\",\n",
    "             bbox=dict(boxstyle=\"round\", alpha=0.15))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74c6b3",
   "metadata": {},
   "source": [
    "## Section 7 ‚Äî Ground Truth: Retrain from Scratch on $D_r$\n",
    "\n",
    "This is the **gold-standard** for machine unlearning. We train a completely fresh model on only the retain set $D_r$, giving us:\n",
    "\n",
    "$$P(\\theta \\mid D_r) \\propto P(D_r \\mid \\theta) \\cdot P(\\theta)$$\n",
    "\n",
    "This is what the MCU unlearned posterior should approximate. We compare:\n",
    "\n",
    "| Model | Trained on | Samples |\n",
    "|---|---|---|\n",
    "| **Base** | $D = D_r \\cup D_e$ | 1500 |\n",
    "| **Unlearned (MCU)** | $D$, reweighted via IS | 1500 (weighted) |\n",
    "| **Retrained** | $D_r$ only | 1500 |\n",
    "\n",
    "The retrained model is computationally expensive (full MCMC run) but gives us the exact target distribution for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retraining from scratch on D_r (ground truth)...\")\n",
    "retrained_samples = train_mcmc(X_r, y_r, num_warmup=500, num_samples=1500,\n",
    "                                rng_key=random.PRNGKey(2))\n",
    "\n",
    "b_base_np      = np.array(base_samples[\"b\"])\n",
    "b_retrained_np = np.array(retrained_samples[\"b\"])\n",
    "b_unlearned_mean = np.average(b_base_np, weights=weights_np)\n",
    "\n",
    "print(f\"\\nBias b posterior summary:\")\n",
    "print(f\"  Base      mean: {b_base_np.mean():.4f}  std: {b_base_np.std():.4f}\")\n",
    "print(f\"  Unlearned mean: {b_unlearned_mean:.4f}  (weighted from base)\")\n",
    "print(f\"  Retrained mean: {b_retrained_np.mean():.4f}  std: {b_retrained_np.std():.4f}\")\n",
    "\n",
    "# --- Overlay MCMC traces: Base vs Retrained ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(b_base_np,      alpha=0.65, color=\"#4C72B0\", linewidth=0.6, label=\"Base (on D)\")\n",
    "axes[0].plot(b_retrained_np, alpha=0.65, color=\"#55A868\", linewidth=0.6, label=\"Retrained (on D_r)\")\n",
    "axes[0].axhline(b_unlearned_mean,        color=\"#4C72B0\", linestyle=\"--\",\n",
    "                linewidth=1.8, label=f\"Unl. mean = {b_unlearned_mean:.3f}\")\n",
    "axes[0].axhline(b_retrained_np.mean(),   color=\"#55A868\", linestyle=\"--\",\n",
    "                linewidth=1.8, label=f\"Ret. mean = {b_retrained_np.mean():.3f}\")\n",
    "axes[0].set_title(\"MCMC Trace ‚Äî Bias $b$ (Base vs Retrained)\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Sample index\")\n",
    "axes[0].set_ylabel(\"$b$\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# KDE overlay for b\n",
    "z_range = np.linspace(\n",
    "    min(b_base_np.min(), b_retrained_np.min()) - 0.5,\n",
    "    max(b_base_np.max(), b_retrained_np.max()) + 0.5, 300)\n",
    "kde_base_b = scipy.stats.gaussian_kde(b_base_np)\n",
    "kde_ret_b  = scipy.stats.gaussian_kde(b_retrained_np)\n",
    "kde_unl_b  = scipy.stats.gaussian_kde(b_base_np, weights=weights_np)\n",
    "\n",
    "axes[1].plot(z_range, kde_base_b(z_range), \"--\", color=\"#4C72B0\", linewidth=2,\n",
    "             label=\"Base $P(b|D)$\")\n",
    "axes[1].plot(z_range, kde_unl_b(z_range),  \"-\",  color=\"#FF5722\", linewidth=2.5,\n",
    "             label=\"Unlearned $P(b|D_r)$ ‚âà IS\")\n",
    "axes[1].plot(z_range, kde_ret_b(z_range),  \":\",  color=\"#55A868\", linewidth=2,\n",
    "             label=\"Retrained $P(b|D_r)$ (truth)\")\n",
    "axes[1].set_title(\"Posterior KDE ‚Äî Bias $b$: How close is Unlearned to Retrained?\",\n",
    "                  fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"$b$\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e035db",
   "metadata": {},
   "source": [
    "## Section 8 ‚Äî Predictive Accuracy Comparison\n",
    "\n",
    "We evaluate all three models on the held-out test set (200 samples) using:\n",
    "\n",
    "$$\\hat{y} = \\mathbb{1}\\!\\left[\\frac{1}{N}\\sum_{i=1}^N \\sigma(\\theta_i^\\top x^*) > 0.5\\right] \\quad \\text{(Base / Retrained)}$$\n",
    "\n",
    "$$\\hat{y} = \\mathbb{1}\\!\\left[\\sum_{i=1}^N \\tilde{w}_i \\cdot \\sigma(\\theta_i^\\top x^*) > 0.5\\right] \\quad \\text{(Unlearned)}$$\n",
    "\n",
    "### Key Insight\n",
    "**Accuracy alone cannot confirm unlearning.** A model can \"forget\" training data while keeping the same test accuracy ‚Äî the test set was never exposed to $D_e$. The Wasserstein distance in Section 9 is the real verification metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, posterior_samples, weights=None):\n",
    "    w = posterior_samples['w']\n",
    "    b = posterior_samples['b']\n",
    "    logits = jnp.dot(w, jnp.array(X, dtype=jnp.float32).T) + b[:, None]\n",
    "    probs = jax.nn.sigmoid(logits)\n",
    "    if weights is None:\n",
    "        mean_probs = jnp.mean(probs, axis=0)\n",
    "    else:\n",
    "        mean_probs = jnp.sum(probs * weights[:, None], axis=0)\n",
    "    return np.array((mean_probs > 0.5).astype(int)), np.array(mean_probs)\n",
    "\n",
    "y_pred_base,      p_base = predict(X_test, base_samples)\n",
    "y_pred_unlearned, p_unl  = predict(X_test, base_samples, weights=unlearning_weights)\n",
    "y_pred_retrained, p_ret  = predict(X_test, retrained_samples)\n",
    "\n",
    "acc_base      = accuracy_score(y_test, y_pred_base)\n",
    "acc_unlearned = accuracy_score(y_test, y_pred_unlearned)\n",
    "acc_retrained = accuracy_score(y_test, y_pred_retrained)\n",
    "\n",
    "print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"‚îÇ Model                            ‚îÇ Accuracy ‚îÇ\")\n",
    "print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "print(f\"‚îÇ Base (trained on D)              ‚îÇ  {acc_base:.4f}  ‚îÇ\")\n",
    "print(f\"‚îÇ Unlearned (MCU IS-weighted)      ‚îÇ  {acc_unlearned:.4f}  ‚îÇ\")\n",
    "print(f\"‚îÇ Retrained (ground truth)         ‚îÇ  {acc_retrained:.4f}  ‚îÇ\")\n",
    "print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "\n",
    "# --- Plots ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "fig.suptitle(\"Predictive Accuracy Comparison\", fontweight=\"bold\")\n",
    "\n",
    "# Accuracy bar\n",
    "models = [\"Base\", \"Unlearned\\n(MCU)\", \"Retrained\\n(GT)\"]\n",
    "accs   = [acc_base, acc_unlearned, acc_retrained]\n",
    "colors = [\"#4C72B0\", \"#DD8452\", \"#55A868\"]\n",
    "bars = axes[0].bar(models, accs, color=colors, edgecolor=\"white\", width=0.5)\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Test Accuracy\")\n",
    "for bar, acc in zip(bars, accs):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, acc + 0.01,\n",
    "                 f\"{acc:.4f}\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# P(y=1|x) histogram\n",
    "bins = np.linspace(0, 1, 30)\n",
    "for ax, probs, name, col in [\n",
    "    (axes[1], p_base, \"Base\",      \"#4C72B0\"),\n",
    "    (axes[1], p_unl,  \"Unlearned\", \"#DD8452\"),\n",
    "    (axes[1], p_ret,  \"Retrained\", \"#55A868\")\n",
    "]:\n",
    "    ax.hist(probs, bins=bins, alpha=0.5, color=col, label=name, edgecolor=\"white\", linewidth=0.3)\n",
    "axes[1].axvline(0.5, color=\"red\", linestyle=\"--\", linewidth=1.5, label=\"Decision (0.5)\")\n",
    "axes[1].set_title(\"Predictive $P(y=1\\\\mid x)$ Distributions\")\n",
    "axes[1].set_xlabel(\"Mean predicted probability\")\n",
    "axes[1].set_ylabel(\"Count (test points)\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "# Confusion matrix for unlearned model\n",
    "cm = confusion_matrix(y_test, y_pred_unlearned)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Legit (0)\", \"Phish (1)\"])\n",
    "disp.plot(ax=axes[2], colorbar=False, cmap=\"Blues\")\n",
    "axes[2].set_title(\"Confusion Matrix ‚Äî Unlearned Model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340538c4",
   "metadata": {},
   "source": [
    "## Section 9 ‚Äî Wasserstein Distance: Unlearning Fidelity\n",
    "\n",
    "The **1-Wasserstein distance** ($W_1$) ‚Äî also called the Earth Mover's Distance ‚Äî measures the \"work\" needed to transform one distribution into another:\n",
    "\n",
    "$$W_1(P_{\\text{unlearned}},\\, P_{\\text{retrained}}) = \\inf_{\\gamma \\in \\Gamma(P_u, P_r)} \\int_{\\Theta \\times \\Theta} \\|\\theta - \\theta'\\| \\; d\\gamma(\\theta, \\theta')$$\n",
    "\n",
    "For 1D empirical distributions, this simplifies to comparing sorted sample quantiles:\n",
    "\n",
    "$$W_1 = \\int_0^1 \\left|F_u^{-1}(t) - F_r^{-1}(t)\\right| dt$$\n",
    "\n",
    "where $F_u^{-1}$ uses the IS weights $\\tilde{w}_i$ for the unlearned model, and uniform weights for retrained.\n",
    "\n",
    "We compute $W_1$ per parameter dimension ($w_0, \\ldots, w_{14}, b$) then report the mean.\n",
    "\n",
    "### Interpretation Guide\n",
    "| $W_1$ | Quality |\n",
    "|---|---|\n",
    "| $\\approx 0$ | Perfect ‚Äî distributions identical |\n",
    "| $< 0.05$ | Excellent unlearning |\n",
    "| $0.05 - 0.2$ | Good approximation |\n",
    "| $> 0.5$ | Poor ‚Äî $D_e$ influence still present |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e73f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "\n",
    "for param_name in ['w', 'b']:\n",
    "    s_unl = np.array(base_samples[param_name])\n",
    "    s_ret = np.array(retrained_samples[param_name])\n",
    "    if s_unl.ndim == 1:\n",
    "        s_unl = s_unl.reshape(-1, 1)\n",
    "        s_ret = s_ret.reshape(-1, 1)\n",
    "    for d in range(s_unl.shape[1]):\n",
    "        wd = scipy.stats.wasserstein_distance(\n",
    "            u_values=s_unl[:, d],\n",
    "            v_values=s_ret[:, d],\n",
    "            u_weights=weights_np\n",
    "        )\n",
    "        distances.append(wd)\n",
    "\n",
    "avg_wd = np.mean(distances)\n",
    "\n",
    "print(f\"Average Wasserstein Distance (Unlearned vs Retrained): {avg_wd:.4f}\")\n",
    "if avg_wd < 0.05:\n",
    "    quality = \"Excellent  ‚úì\"\n",
    "elif avg_wd < 0.2:\n",
    "    quality = \"Good\"\n",
    "elif avg_wd < 0.5:\n",
    "    quality = \"Moderate ‚Äî consider Enlarged Candidate Set\"\n",
    "else:\n",
    "    quality = \"Poor ‚úó ‚Äî weight degeneracy likely\"\n",
    "print(f\"Unlearning quality: {quality}\")\n",
    "\n",
    "# --- Per-dimension bar chart ---\n",
    "n_w = np.array(base_samples[\"w\"]).shape[1]\n",
    "dim_labels = [f\"w[{i}]\" for i in range(n_w)] + [\"b\"]\n",
    "bar_colors = [\"#c0392b\" if d > avg_wd else \"#2980b9\" for d in distances]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "bars = axes[0].bar(dim_labels, distances, color=bar_colors, edgecolor=\"white\", linewidth=0.4)\n",
    "axes[0].axhline(avg_wd, color=\"black\", linestyle=\"--\", linewidth=1.8,\n",
    "                label=f\"Mean $W_1$ = {avg_wd:.4f}\")\n",
    "axes[0].set_title(\"Per-Dimension Wasserstein Distance\\n(Unlearned vs Retrained)\",\n",
    "                  fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Parameter dimension\")\n",
    "axes[0].set_ylabel(\"$W_1$ distance\")\n",
    "axes[0].set_xticklabels(dim_labels, rotation=60, ha=\"right\", fontsize=7)\n",
    "axes[0].legend(handles=[\n",
    "    Patch(color=\"#c0392b\", label=\"Above mean\"),\n",
    "    Patch(color=\"#2980b9\", label=\"Below mean\"),\n",
    "    Line2D([0],[0], color=\"black\", linestyle=\"--\", label=f\"Mean = {avg_wd:.4f}\")\n",
    "], fontsize=8)\n",
    "\n",
    "# Posterior KDE for top-3 most-shifted dimensions\n",
    "top3_shifted = np.argsort(distances[:-1])[-3:][::-1]\n",
    "palette_kde  = [\"#2196F3\", \"#4CAF50\", \"#FF9800\"]\n",
    "w_base_np    = np.array(base_samples[\"w\"])\n",
    "w_ret_np     = np.array(retrained_samples[\"w\"])\n",
    "\n",
    "for rank, dim in enumerate(top3_shifted):\n",
    "    col      = palette_kde[rank]\n",
    "    x_b      = w_base_np[:, dim]\n",
    "    x_r      = w_ret_np[:, dim]\n",
    "    x_range  = np.linspace(min(x_b.min(), x_r.min()) - 0.3,\n",
    "                            max(x_b.max(), x_r.max()) + 0.3, 300)\n",
    "    kde_b = scipy.stats.gaussian_kde(x_b)\n",
    "    kde_u = scipy.stats.gaussian_kde(x_b, weights=weights_np)\n",
    "    kde_r = scipy.stats.gaussian_kde(x_r)\n",
    "    lbl = f\"w[{dim}]\"\n",
    "    axes[1].plot(x_range, kde_b(x_range), \"--\",  color=col, alpha=0.55, linewidth=1.5,\n",
    "                 label=f\"Base {lbl}\")\n",
    "    axes[1].plot(x_range, kde_u(x_range), \"-\",   color=col, alpha=1.0,  linewidth=2.2,\n",
    "                 label=f\"Unlearned {lbl}\")\n",
    "    axes[1].plot(x_range, kde_r(x_range), \":\",   color=col, alpha=0.75, linewidth=1.5,\n",
    "                 label=f\"Retrained {lbl}\")\n",
    "\n",
    "axes[1].set_title(\"Posterior KDE ‚Äî Top-3 Most-Shifted Weight Dimensions\",\n",
    "                  fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Parameter value\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "custom_handles = [\n",
    "    Line2D([0],[0], linestyle=\"--\", color=\"grey\", lw=1.5, label=\"Base\"),\n",
    "    Line2D([0],[0], linestyle=\"-\",  color=\"grey\", lw=2.2, label=\"Unlearned (IS)\"),\n",
    "    Line2D([0],[0], linestyle=\":\",  color=\"grey\", lw=1.5, label=\"Retrained (GT)\"),\n",
    "] + [Line2D([0],[0], color=palette_kde[i], lw=2, label=f\"w[{top3_shifted[i]}]\")\n",
    "     for i in range(3)]\n",
    "axes[1].legend(handles=custom_handles, fontsize=8, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113ad42",
   "metadata": {},
   "source": [
    "## Section 10 ‚Äî Full Diagnostic Dashboard\n",
    "\n",
    "The `plot_all_diagnostics()` function renders a **2√ó3 diagnostic dashboard** that consolidates all key views into a single exportable figure.\n",
    "\n",
    "| Panel | What it shows | Key question answered |\n",
    "|---|---|---|\n",
    "| **[1] Dataset Split (PCA 2D)** | Retain (blue/orange) vs Erase (red) points | Is D_e clustered or scattered? |\n",
    "| **[2] Importance Weights** | Weight histogram + ESS | Is there weight degeneracy? |\n",
    "| **[3] Posterior KDE** | Base / Unlearned / Retrained marginals (top-3 dims) | Does Unlearned ‚âà Retrained? |\n",
    "| **[4] MCMC Trace (bias b)** | Chain trajectory + weighted mean lines | Did the posterior shift after unlearning? |\n",
    "| **[5] Predictive P(y=1\\|x)** | Predicted probability distributions | Did unlearning preserve utility? |\n",
    "| **[6] Per-Dim W‚ÇÅ** | Bar chart across all 16 dimensions | Which parameters shifted most? |\n",
    "\n",
    "The figure is saved as `mcu_diagnostics_{mode}.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_diagnostics(D_r, D_e, X_test, y_test,\n",
    "                         base_samples, retrained_samples,\n",
    "                         unlearning_weights, distances, mode):\n",
    "    weights_np = np.array(unlearning_weights)\n",
    "    X_r, y_r = D_r\n",
    "    X_e, y_e = D_e\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    fig.suptitle(f\"MCU Diagnostic Dashboard  [{mode.upper()} DELETION MODE]\",\n",
    "                 fontsize=15, fontweight=\"bold\", y=1.01)\n",
    "    gs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.45, wspace=0.35)\n",
    "\n",
    "    # Panel 1: PCA dataset split\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    X_all = np.vstack([X_r, X_e])\n",
    "    y_all = np.concatenate([y_r, y_e])\n",
    "    labs  = np.array([\"Retain\"]*len(X_r) + [\"Erase\"]*len(X_e))\n",
    "    pca = PCA(n_components=2)\n",
    "    X2d = pca.fit_transform(X_all)\n",
    "    vr  = pca.explained_variance_ratio_\n",
    "    for cls, marker, color in [(0,\"o\",\"#4C72B0\"),(1,\"s\",\"#DD8452\")]:\n",
    "        m = y_all == cls\n",
    "        ax1.scatter(X2d[m & (labs==\"Retain\"), 0], X2d[m & (labs==\"Retain\"), 1],\n",
    "                    marker=marker, c=color, alpha=0.3, s=15, label=f\"Retain cls={cls}\")\n",
    "        em = m & (labs==\"Erase\")\n",
    "        if em.any():\n",
    "            ax1.scatter(X2d[em, 0], X2d[em, 1], marker=marker, c=\"red\",\n",
    "                        edgecolors=\"black\", linewidths=0.8, alpha=0.9, s=70,\n",
    "                        label=f\"Erase cls={cls}\")\n",
    "    ax1.set_title(\"[1] Dataset Split (PCA 2D)\", fontweight=\"bold\")\n",
    "    ax1.set_xlabel(f\"PC1 ({vr[0]*100:.1f}%)\")\n",
    "    ax1.set_ylabel(f\"PC2 ({vr[1]*100:.1f}%)\")\n",
    "    ax1.legend(fontsize=7)\n",
    "    ax1.text(0.02, 0.97, f\"D_r={len(X_r)}  D_e={len(X_e)}\",\n",
    "             transform=ax1.transAxes, va=\"top\", fontsize=8,\n",
    "             bbox=dict(boxstyle=\"round\", alpha=0.15))\n",
    "\n",
    "    # Panel 2: Importance weights\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    N   = len(weights_np)\n",
    "    ess = 1.0 / np.sum(weights_np**2)\n",
    "    ess_pct = ess/N*100\n",
    "    ax2.hist(weights_np, bins=60, color=\"steelblue\", edgecolor=\"white\", linewidth=0.4)\n",
    "    ax2.axvline(1/N, color=\"crimson\", linestyle=\"--\", lw=1.5, label=f\"Uniform=1/N\")\n",
    "    ax2.set_title(\"[2] Importance Weights\", fontweight=\"bold\")\n",
    "    ax2.set_xlabel(\"$\\\\tilde{w}_i$\"); ax2.set_ylabel(\"Count\")\n",
    "    ax2.legend(fontsize=8)\n",
    "    ax2.text(0.97, 0.93, f\"ESS={ess:.0f}/{N}\\n({ess_pct:.1f}%)\",\n",
    "             transform=ax2.transAxes, ha=\"right\", va=\"top\", color=\"darkred\",\n",
    "             fontweight=\"bold\", fontsize=9, bbox=dict(boxstyle=\"round\", alpha=0.15))\n",
    "    if ess_pct < 10:\n",
    "        ax2.set_facecolor(\"#fff0f0\")\n",
    "        ax2.text(0.5, 0.5, \"‚ö† Degeneracy\", transform=ax2.transAxes,\n",
    "                 fontsize=12, ha=\"center\", color=\"red\", alpha=0.35,\n",
    "                 fontweight=\"bold\", rotation=12)\n",
    "\n",
    "    # Panel 3: Posterior KDE\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    wb = np.array(base_samples[\"w\"])\n",
    "    wr = np.array(retrained_samples[\"w\"])\n",
    "    top3 = np.argsort(np.abs(wb.mean(axis=0)))[-3:][::-1]\n",
    "    pal  = [\"#2196F3\",\"#4CAF50\",\"#FF9800\"]\n",
    "    for rank, dim in enumerate(top3):\n",
    "        col = pal[rank]\n",
    "        xb = wb[:, dim]; xr = wr[:, dim]\n",
    "        xrng = np.linspace(min(xb.min(),xr.min())-0.3, max(xb.max(),xr.max())+0.3, 300)\n",
    "        ax3.plot(xrng, scipy.stats.gaussian_kde(xb)(xrng),           \"--\", color=col, alpha=0.6)\n",
    "        ax3.plot(xrng, scipy.stats.gaussian_kde(xb,weights=weights_np)(xrng), \"-\", color=col, lw=2)\n",
    "        ax3.plot(xrng, scipy.stats.gaussian_kde(xr)(xrng),           \":\",  color=col, alpha=0.8)\n",
    "    ax3.set_title(\"[3] Posterior Marginals (Top-3 Weights)\", fontweight=\"bold\")\n",
    "    ax3.set_xlabel(\"Value\"); ax3.set_ylabel(\"Density\")\n",
    "    ax3.legend(handles=[\n",
    "        Line2D([0],[0], ls=\"--\", color=\"grey\", label=\"Base\"),\n",
    "        Line2D([0],[0], ls=\"-\",  color=\"grey\", lw=2, label=\"Unlearned\"),\n",
    "        Line2D([0],[0], ls=\":\",  color=\"grey\", label=\"Retrained\"),\n",
    "    ] + [Line2D([0],[0], color=pal[i], lw=2, label=f\"w[{top3[i]}]\") for i in range(3)],\n",
    "        fontsize=7, ncol=2)\n",
    "\n",
    "    # Panel 4: MCMC trace for b\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    bb = np.array(base_samples[\"b\"]); br = np.array(retrained_samples[\"b\"])\n",
    "    ax4.plot(bb, alpha=0.65, color=\"#4C72B0\", lw=0.6, label=\"Base\")\n",
    "    ax4.plot(br, alpha=0.65, color=\"#55A868\", lw=0.6, label=\"Retrained\")\n",
    "    ax4.axhline(np.average(bb, weights=weights_np), color=\"#4C72B0\", ls=\"--\", lw=1.8, label=\"Unl. mean\")\n",
    "    ax4.axhline(br.mean(), color=\"#55A868\", ls=\"--\", lw=1.8, label=\"Ret. mean\")\n",
    "    ax4.set_title(\"[4] MCMC Trace ‚Äî Bias $b$\", fontweight=\"bold\")\n",
    "    ax4.set_xlabel(\"Sample index\"); ax4.set_ylabel(\"$b$\")\n",
    "    ax4.legend(fontsize=8)\n",
    "\n",
    "    # Panel 5: Predictive P(y=1|x)\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    wb_np = np.array(base_samples[\"w\"]); bb_np = np.array(base_samples[\"b\"])\n",
    "    wr_np = np.array(retrained_samples[\"w\"]); br_np = np.array(retrained_samples[\"b\"])\n",
    "    pb = 1/(1+np.exp(-(np.dot(wb_np, X_test.T)+bb_np[:,None])))\n",
    "    bins = np.linspace(0, 1, 30)\n",
    "    ax5.hist(pb.mean(axis=0),                          bins=bins, alpha=0.5, color=\"#4C72B0\", label=\"Base\")\n",
    "    ax5.hist((pb*weights_np[:,None]).sum(axis=0),       bins=bins, alpha=0.5, color=\"#DD8452\", label=\"Unlearned\")\n",
    "    ax5.hist((1/(1+np.exp(-(np.dot(wr_np, X_test.T)+br_np[:,None])))).mean(axis=0),\n",
    "             bins=bins, alpha=0.5, color=\"#55A868\", label=\"Retrained\")\n",
    "    ax5.axvline(0.5, color=\"red\", ls=\"--\", lw=1.2, alpha=0.8, label=\"0.5\")\n",
    "    ax5.set_title(\"[5] Predictive $P(y=1\\\\mid x)$\", fontweight=\"bold\")\n",
    "    ax5.set_xlabel(\"Mean probability\"); ax5.set_ylabel(\"Count\")\n",
    "    ax5.legend(fontsize=8)\n",
    "\n",
    "    # Panel 6: Per-dim Wasserstein\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    n_w = np.array(base_samples[\"w\"]).shape[1]\n",
    "    dlabels = [f\"w[{i}]\" for i in range(n_w)] + [\"b\"]\n",
    "    dcolors = [\"#c0392b\" if d > np.mean(distances) else \"#2980b9\" for d in distances]\n",
    "    ax6.bar(dlabels, distances, color=dcolors, edgecolor=\"white\", linewidth=0.4)\n",
    "    ax6.axhline(np.mean(distances), color=\"black\", ls=\"--\", lw=1.8,\n",
    "                label=f\"Mean={np.mean(distances):.4f}\")\n",
    "    ax6.set_title(\"[6] Per-Dim Wasserstein Distance\", fontweight=\"bold\")\n",
    "    ax6.set_xlabel(\"Dimension\"); ax6.set_ylabel(\"$W_1$\")\n",
    "    ax6.set_xticklabels(dlabels, rotation=60, ha=\"right\", fontsize=7)\n",
    "    ax6.legend(handles=[Patch(color=\"#c0392b\", label=\"Above mean\"),\n",
    "                        Patch(color=\"#2980b9\", label=\"Below mean\"),\n",
    "                        Line2D([0],[0], color=\"black\", ls=\"--\",\n",
    "                               label=f\"Mean={np.mean(distances):.4f}\")], fontsize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fname = f\"mcu_diagnostics_{mode}.png\"\n",
    "    plt.savefig(fname, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"[Diagnostics saved ‚Üí {fname}]\")\n",
    "    plt.show()\n",
    "\n",
    "# Run the full dashboard\n",
    "plot_all_diagnostics(D_r, D_e, X_test, y_test,\n",
    "                     base_samples, retrained_samples,\n",
    "                     unlearning_weights, distances, active_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9abba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Conclusions\n",
    "\n",
    "### What We Did\n",
    "1. **Generated** a 1000-sample binary classification dataset (phishing website analogue)\n",
    "2. **Split** training data into retain set $D_r$ and erase set $D_e$ (random or cluster mode)\n",
    "3. **Trained** a Bayesian Logistic Regression model via NUTS MCMC on all of $D$\n",
    "4. **Applied MCU** ‚Äî reweighted the posterior using importance sampling to approximate $P(\\theta | D_r)$ *without* any additional sampling\n",
    "5. **Verified** the result against a full retrain from scratch using Wasserstein distance\n",
    "\n",
    "### MCU vs Full Retrain\n",
    "| Criterion | MCU | Full Retrain |\n",
    "|---|---|---|\n",
    "| Compute cost | O(N ¬∑ \\|D_e\\|) ‚Äî cheap | O(full MCMC run) ‚Äî expensive |\n",
    "| Exactness | Approximation | Exact |\n",
    "| Scales to large D_e | ‚ö† Needs Enlarged Candidate Set | ‚úì Always works |\n",
    "| Scales to large D | ‚úì Same cost regardless of D size | ‚úó Cost grows with D |\n",
    "\n",
    "### Key Takeaway\n",
    "> A small Wasserstein distance ($W_1 < 0.05$) combined with unchanged test accuracy confirms that MCU successfully **approximates full retraining at a fraction of the cost**.\n",
    "\n",
    "### References\n",
    "- Nguyen et al., *\"MCMC-Based Machine Unlearning: Unlearning what needs to be forgotten\"*, ACM ASIACCS 2022  \n",
    "- Hoffman & Gelman, *\"The No-U-Turn Sampler\"*, JMLR 2014  \n",
    "- Phan et al., *NumPyro: Composable Effects for Flexible and Accelerated Probabilistic Programming*, 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
